@misc{2022kingma,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{2021doersch,
      title={Tutorial on Variational Autoencoders}, 
      author={Carl Doersch},
      year={2021},
      eprint={1606.05908},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{2021kim,
  title = 	 {Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech},
  author =       {Kim, Jaehyeon and Kong, Jungil and Son, Juhee},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5530--5540},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21f/kim21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kim21f.html},
  abstract = 	 {Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.}
}

@inproceedings{2020kim,
	author = {Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {8067--8077},
	publisher = {Curran Associates, Inc.},
	title = {Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/5c3b99e8f92532e5ad1556e53ceea00c-Paper.pdf},
	volume = {33},
	year = {2020}
}

@article{takamichi2019jvs,
  title={JVS Corpus: Free Japanese Multi-Speaker Voice Corpus},
  author={Takamichi, Shinnosuke and Mitsui, Kentaro and Saito, Yuki and Koriyama, Tomoki and Tanji, Naoko and Saruwatari, Hiroshi},
  journal={arXiv preprint arXiv:1908.06248},
  month={Aug},
  year={2019}
}
@inproceedings{AISHELL-3_2020,
  title={AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines},
  author={Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, Ming Li},
  year={2015},
  url={https://arxiv.org/abs/2010.11567}
}