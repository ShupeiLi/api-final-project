\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{siunitx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./img/} }

\newcommand{\note}[1]{\textbf{#1}}

\title{Towards a Customizable Text-to-speech Personal Assistant}


\author{
 Chenyu Shi\\
   s3500063\\
   \And
 Siwen Tu\\
   s3631400\\
   \And
 Shuang Fan \\
   s3505847\\
   \And
 Shupei Li \\
   s3430863\\
}


\begin{document}
\maketitle
\begin{abstract}
The audio assistant technology has been widely applied based on the high development of the deep learning field. In this project, we have used deep learning models to accomplish a customizable text-to-speech personal audio assistant. We trained the neural networks on audio datasets, and fine-tune them on different languages of multi speakers. The final model can produce realistic audio based on input texts, and users can customize the type of languages or speakers according to their choice.
\end{abstract}


\keywords{VITS \and TTS \and Deep Learning}


\section{Introduction}\label{sec:intro}
With the rapid development of the deep learning field, the audio assistant technology has achieved great success and been widely applied in our daily life. Most of audio assistants are realized by text-to-speech models. In this project, we accomplished a customizable text-to-speech personal audio assistant based on VITS \cite{2021kim}, an efficient text-to-speech model with high performance. We trained the model on audio datasets, and fine-tune it on Mandarin and Japanese to test the performance of the model on different languages. Besides, we also fine-tune it on multiple types of speakers to fulfill the requirement of customization. The final model achieves great performance, which can produce realistic audio from text, and the users can make their choice on different languages and types of speakers.

The rest of the reports will be presented in three chapters. In chapter 2,  we will show and explain the methodology used in the models. In chapter 3, the experiment results will be displayed and discussed. And in chapter 4, we will have a conclusion of the report and project.
\section{Methodology}
\label{sec:method}

We describe VITS \cite{2021kim}, the backbone of our TTS assistant, from the perspective of variational inference in detail. We also introduce VITS's architecture and training process to provide a comprehensive overview.

\subsection{CVAEs}
The variational autoencoder (VAE), proposed in \cite{2022kingma}, is a popular approach widely used in unsupervised learning. It is an effective function approximator and can be optimized by the standard stochastic gradient descent (SGD) method. It has shown promising application value in various fields, such as complex pattern recognition, segmentation, future prediction from static images, etc \cite{2021doersch}.

Conditional variational autoencoders (CVAEs) are variants of VAEs. Remember that VAEs learn posterior distribution parameters from dataset $x$ without any label information. In CVAEs, we have additional contextual information $c$ when estimating the posterior distribution. The objective function of the CVAE can be written as follows:
\begin{align}
    \max\quad \log p_{\theta}\left(x|c\right) &- \mathcal{D}\left[q_{\phi} \left(z|x\right)\Vert p_{\theta}\left(z|x \right) \right] 
    \label{eq:objective-function}
\end{align}
where $p_\theta(x|c)$ denotes the marginal log-likelihood of the data, $p_\theta(z|x)$ is the posterior distribution and $q_\phi(z|x)$ is its approximator. According to the definition of Kullback-Leibler divergence and Bayes' theorem, maximizing Equation \ref{eq:objective-function} is equivalent to maximizing the following equation:
\begin{align}
    \max\quad \mathbb{E}_{q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)-\log \frac{q_{\phi}(z \mid x)}{p_{\theta}(z \mid c)}\right]
    \label{eq:objective-function-equal}
\end{align}
Notice that the relative KL divergence is always a non-negative number. We can derive the evidence lower bound (ELBO) of the objective function based on Equation \ref{eq:objective-function} and Equation \ref{eq:objective-function-equal}:
\begin{align}
    \begin{split}
        \log p_{\theta}(x \mid c) &\geq \mathbb{E}_{q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)-\log \frac{q_{\phi}(z \mid x)}{p_{\theta}(z \mid c)}\right]\\
                                  &= \underbrace{\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}\left(x|z \right)\right]}_{\text{Reconstruction loss}} - \underbrace{\mathcal{D}\left[q_{\phi}(z \mid x) \Vert p_{\theta}(z \mid c) \right]}_{\text{Regularization loss}}
    \end{split}
    \label{eq:elbo}
\end{align}
The ELBO comprises two parts --- the reconstruction loss and the regularization loss. The reconstruction loss describes the data distribution given the latent space, while the regularization loss measures the divergence between the true prior distribution and the encoder's approximate distribution. In practice, we usually choose to minimize the negative ELBO because the term $\mathcal{D}\left[q_{\phi} \left(z|x\right)\Vert p_{\theta}\left(z|x \right) \right]$ is hard to calculate. However, a reasonable posterior approximator $q_\phi(z|x)$ can effectively alleviate the impact of the bias.

\subsection{Expressing VITS as a CVAE}
\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=0.95\textwidth]{vits.png}
    \end{center}
    \caption{The architecture of VITS. This figure is directly adapted from \cite{2021kim}.}
    \label{fig:method-vits}
\end{figure}

The architecture of VITS is shown in Figure \ref{fig:method-vits}. VITS model can be expressed as a CVAE. We will illustrate the main idea behind VITS based on the reconstruction loss as well as the regularization loss.

\textbf{Reconstruction Loss}. During the training, we have ground truth soundtracks $x$ and their corresponding text $c_{text}$. Kim et al. \cite{2021kim} define the reconstruction loss as the $L_1$ norm between the input's mel-spectrogram and the estimated mel-spectrogram produced by the model:
\begin{align}
    L_{recon} = \Vert x_{mel} - \hat{x}_{mel} \Vert_1
\end{align}
It is easy to compute the mel-spectrogram $x_{mel}$ of a specific audio file. As for the approximate mel-spectrogram $\hat{x}_{mel}$, we can upsample the latent space to the waveform domain and then transform it to the mel-spectrogram domain.

\textbf{Regularization Loss}. The text-to-speech task requires an automatic alignment between the text input and the corresponding voice features. VITS adopts the monotonic alignment search algorithm to align the audio and textual embeddings. The monotonic alignment search algorithm (MAS) is proposed in \cite{2020kim}. The intuition behind the MAS is applying the dynamic programming algorithm to maximize the likelihood of the data distribution. It will produce an alignment matrix $A$. The condition $c$ in VITS is defined as the concatenation of the text input $c_{text}$ and the alignment matrix $A$, i.e., $c = [c_{text}, A]$. Given the condition $c$, we can calculate the regularization loss following the definition of KL-divergence:
\begin{align}
    \begin{split}
        L_{kl} = \mathcal{D}\left[q_{\phi}(z \mid x) \Vert p_{\theta}(z \mid c) \right]&=\log q_{\phi}\left(z \mid x_{l i n}\right)-\log p_{\theta}\left(z \mid c_{\text {text }}, A\right) \\
        z \sim q_{\phi}\left(z \mid x_{l i n}\right)&=N\left(z ; \mu_{\phi}\left(x_{l i n}\right), \sigma_{\phi}\left(x_{l i n}\right)\right)
    \end{split}
\end{align}
It is worth mentioning that VITS uses the linear-scale spectrogram of the soundtrack $x_{lin}$ instead of the mel-spectrogram. \cite{2021kim} explains that the linear-scale spectrogram is helpful in enhancing the resolution of generated audio. To improve the model's performance further, VITS introduces a normalizing flow $f_\theta$ that connects the conditional prior distribution and the latent space in the following way:
\begin{align}
    p_{\theta}(z \mid c) & =N\left(f_{\theta}(z) ; \mu_{\theta}(c), \sigma_{\theta}(c)\right)\left|\operatorname{det} \frac{\partial f_{\theta}(z)}{\partial z}\right| 
    \label{eq:normalizing-flow}
\end{align}
Equation \ref{eq:normalizing-flow} transforms the latent space into some complex distributions that possess a more powerful expressiveness, which is critical for generating realistic outputs.

\subsection{Training VITS}
The major part of VITS is a CVAE. However, there are more things to consider when handling the text-to-speech task. Firstly, we need to decide the duration of each token in outputs. A straightforward idea is adding a deterministic duration predictor into the model. But this naive method has a big flaw --- it can not reflect the features of different speakers, which limits the model's application in multi-speaker scenarios. Alternatively, VITS designs a stochastic duration preditor whose loss function is $L_{dur}$ and optimizes it with other modules during training.

VITS also integrates adversarial training into the architecture to guide the optimization direction. With a discriminator $D$ and a decoder $G$, VITS combines two kinds of loss in the adversarial training, namely the least-squares loss and the additional feature-matching loss:
\begin{align}
    \begin{split}
        L_{a d v}(D) & =\mathbb{E}_{(y, z)}\left[(D(y)-1)^{2}+(D(G(z)))^{2}\right] \\
        L_{a d v}(G) & =\mathbb{E}_{z}\left[(D(G(z))-1)^{2}\right] \\
        L_{f m}(G) & =\mathbb{E}_{(y, z)}\left[\sum_{l=1}^{T} \frac{1}{N_{l}}\left\|D^{l}(y)-D^{l}(G(z))\right\|_{1}\right]
    \end{split}
\end{align}

Taken together, we can obtain the final loss for VITS training:
\begin{align}
    L_{v a e}=L_{r e c o n}+L_{k l}+L_{d u r}+L_{a d v}(G)+L_{f m}(G)
\end{align}

During the inference stage, we fix all weights and disgard the posterior encoder because we only have the text input. Final outputs are produced by the decoder $G$.

\section{Experiments}
\label{sec:exper}

\section{Conclusion}
To conclude, we have built a customizable text-to-speech personal audio assistant based on the VITS model and fine-tune it to achieve optimal performance. Although there still exist some problems, such as equivalent performance. For example, audios from some kind of speaker are of higher quality than others. But overally speaking, the model has achieved the goal and performs well as a personal audio assistant.
\label{sec:con}



\bibliographystyle{unsrt}  
\bibliography{references} 

\end{document}
